"""
The implementation of experiment 2 
"""
import os
from datetime import datetime
import time
import numpy as np
import random
import json
from tqdm import tqdm
import math
import copy

import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from torchinfo import summary

import utils
from utils import flatten_list, flatten_onehot_profile
import generate_data
from generate_data import generate_profile_data, pad_profile_data
from generate_data import onehot_profile_data
import models
from models import MLP, CNN, WEC
import train_and_eval

from gensim.models import Word2Vec


import pref_voting
from pref_voting.profiles import Profile





def experiment2(
        architecture,
        rule_names,
        max_num_voters,
        max_num_alternatives,
        election_sampling,
        batch_composition,
        num_gradient_steps,
        eval_dataset_size,
        sample_size_applicable,
        sample_size_maximal,
        architecture_parameters=None,
        axioms_check_model = list(utils.dict_axioms.keys()),
        axioms_check_rule = list(utils.dict_axioms_rules.keys()),
        merge="accumulative",
        comparison_rules = None,
        compute_resoluteness = False,
        random_seed=None,
        report_intervals = {'plot':100,'print':1000},
        learning_rate = 1e-3,
        learning_scheduler = None,
        weight_decay = 0,
        save_model=False,
        load_model_from = None,
        verbose = False,
    ):
    """
    Implements our experiment 2

    Inputs:
    * `architecture` can be either `MLP`, `CNN`, or `WEC`. The latter two 
      require additional parameters which can be passed as 
      `architecture_parameters` below (otherwise default values are chosen).

    * `rule_names`: list of names of rules used for data generation. Typically 
      just a singleton list. If multiple rules are given, their output is 
      merged in the dataset. 
    * `max_num_voters`: a positive integer describing the maximal number of 
      voters that will be considered
    * `max_num_alternatives`: a positive integer describing the maximal number 
      of alternatives that will be considered
    * `election_sampling`: a dictionary describing the parameters for the
      probability model with which profiles are generated. The most important 
      key is `probmodel`. See 
      https://pref-voting.readthedocs.io/en/latest/generate_profiles.html

    * `batch_composition` is a dictionary of the following form
      {
        'batch_size':100,
        'proper_data':50,
        'neut_generated':25,
        'anon_generated':25
      }
      This describes how the training data batches are generated. Each batch 
      consists of `batch_size` many pairs of a profile and a winning set. This 
      is done as follows. First, `proper_data` many profiles are sampled 
      according to `election_sampling` and their winning sets computed with 
      `rule_names`. Then 'neut_generated' many profiles are generated by 
      randomly picking a proper profile and an alternatives-permutation and add
      the profile and winning sets with alternatives thus permuted. Finally, 
      'anon_generated' many profiles are generated by randomly picking a proper
      profile and a voter-permutation and add the profile and winning sets 
      with voters thus permuted.        
      The constraints are that `proper_data` > 0 and
        'proper_data' + 'neut_generated' + 'anon_generated' = 'batch_size'

    * `num_gradient_steps`: a positive integer describing the number of 
      gradient steps performed during training the model.
    * `eval_dataset_size`: a positive integer describing the number of profiles
      with their corresponding winning sets that should be used for testing.
    
    * `sample_size_applicable`: a positive integer describing the number of 
      sampled profiles on which the axioms are checked and applicable. 
    * `sample_size_maximal`: a positive integer describing how many profiles 
      are at most sampled when trying to find profiles on which the axioms are 
      applicable. 

     * `architecture_parameters` is not needed for MLPs, but CNNs and WEC they 
       are given the following default values
        * For CNN: 
            {
                'kernel1':[5,1], 
                'kernel2':[1,5], 
                'channels':64
            } 
          Here `kernel1`: a list [height, width] of the dimension for the 
          kernel/filter in the first convolutional layer of the model. (The 
          height of the images is max_num_alternatives and the width of the 
          images is max_num_voters.) If max num voters/alternatives are quite 
          small, the kernel cannot be too big, otherwise error will be raised.  
          Similarly, `kernel2` is the [height, width]-dimension for the 
          kernel/filter in the second convolutional layer of the model.
          Finally, `channels` is the number of channels of the feature maps of 
          the model.
        * For WEC: 
            {
                'we_corpus_size':int(1e5), 
                'we_size':100, 
                'we_window':5, 
                'we_algorithm':1,
                'load_embeddings_from':'path'
            } 
          Here 'we_corpus_size' is the number of profiles used to pretrain the 
          word embeddings,  `we_size` is the size (i.e., length of vector) of 
          the word embeddings, `we_window` is the size of the window used when 
          training the word embeddings. Finally, `we_algorithm` is either 0 
          or 1 depending on whether one uses the CBOW algorithm or the 
          skip gram algorithm.  If 'load_embeddings_from' is specified with a 
          path to stored embeddings, then these are loaded rather than 
          computing new ones. 
              
    * `axioms_check_model` is a list of names of axioms whose satisfaction is 
      checked for the trained model. By default, it is set to all axioms. If 
      empty, no axioms are checked.
    * `axioms_check_rule` is a list of names of axioms whose satisfaction is 
      checked for the rules. By default, it is only the condorcet and the 
      independence axiom, since the other axioms are always satisfied for 
      common voting rules. If empty, no axioms are checked.   


    * `merge`: By default set to `accumulative`, but could also be `selective`. 
      If `rule_names` has length 1 (i.e., only a single rule is considered), 
      both are equivalent. If there are several rules, `selective` means a 
      profile is added to the training dataset only if all rules output the 
      same winning set, while with `accumulative` all profiles are added.
    
    * `comparison_rules` is, if not None, a nonempty list of names of voting 
      rules, to which the rule found by the model is compared to with respect 
      to various measures of similarity.
    * `compute_resoluteness` is False by default. If True, the resoluteness 
      coefficient of the learned rule is computed. A rule is highly resolute if
      it picks very few winners, and it is not resolute at all if it always 
      declares all alternatives as winners.

    * `random_seed` (optional): If not None, set all random seeds to this 
      provided value.
    * `report_intervals` by default is the dictionary 
        {'plot':100,'print':1000}, 
      saying that after every 10 (resp., 1000) gradient steps the dev-loss and 
      dev-accuracy is computed to later plot (resp. print) the learning 
      performance.
    * `learning_rate`: a float number describing the learning rate when 
      training the model. The default value is 1e-3.
    * `learning_scheduler`: By default None, but if given, then a positive 
      integer describing the T_0 value for the CosineAnnealingWarmRestarts 
      scheduler (i.e., the number of iterations for the first restart).
    * `weight_decay` of the optimizer which we set by default to 0 since we use 
      synthetic data and hence don't need regularization (its usual default 
      value is 0.01).  
    * `save_model`: By default False, but if true, the neural network model 
      will be saved.
    * If `load_model_from` is given, it should be a path to a folder with  a 
      (previously trained) pytorch model 'model.pth' which is then used instead
      of a randomly initialized one.
    * `verbose` if True, some intermediary results about the experiment are 
      printed.
    """


    # SET UP BASICS

    start_time = time.time()

    assert (
        architecture in ['MLP', 'CNN', 'WEC']
    ), f"The supported architectures are 'MLP', 'CNN', and 'WEC' but {architecture} was given"

    if architecture_parameters is None:
        if architecture == 'CNN':
            architecture_parameters = {
                'kernel1':[5,1] , 
                'kernel2':[1,5], 
                'channels':64} 
        if architecture == 'WEC':
            architecture_parameters = {
                'we_corpus_size':int(1e5),
                'we_size':100, 
                'we_window':5, 
                'we_algorithm':1}

    assert (batch_composition['proper_data'] > 0), f"The amount of proper data per batch must be at least 1"
    assert (
        batch_composition['batch_size'] == batch_composition['proper_data'] + batch_composition['neut_generated'] + batch_composition['anon_generated']
    ), f"The amount of proper_data, neut_generated, and anon_generated data need to sum up to batch_size"

    batch_size = batch_composition['batch_size']

    # Set seeds
    if random_seed is not None:
        random.seed(random_seed)
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        torch.cuda.manual_seed(random_seed)
        torch.use_deterministic_algorithms(True)
        torch.backends.cudnn.benchmark = False

    # Set up saving of results
    rules_short = ""
    for rule_name in rule_names:
        rules_short += rule_name
    prob_model = election_sampling['probmodel']
    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    location = f"./results/exp2/{architecture}/exp2_{current_time}_{rules_short}_{prob_model}"
    os.mkdir(location)
    print(f'Saving location: {location}')


    results = {
        "location": location,        
        "architecture": architecture,
        "rule_names": rule_names,
        "max_num_voters": max_num_voters,
        "max_num_alternatives": max_num_alternatives,
        "election_sampling": election_sampling,
        "batch_composition": batch_composition,
        "num_gradient_steps": num_gradient_steps,
        "eval_dataset_size": eval_dataset_size,
        "sample_size_applicable": sample_size_applicable,
        "sample_size_maximal": sample_size_maximal,
        "architecture_parameters": architecture_parameters,
        "axioms_check_model": axioms_check_model,
        "axioms_check_rule": axioms_check_rule,
        "merge": merge,
        "comparison_rules": comparison_rules,
        "compute_resoluteness": compute_resoluteness,
        "random_seed": random_seed,
        "report_intervals": report_intervals,
        "learning_rate": learning_rate,
        "learning_scheduler": learning_scheduler,
        "weight_decay": weight_decay,
        "save_model": save_model,
        "load_model_from": load_model_from
    }



    with open(f"{location}/results.json", "w") as json_file:
        json.dump(results, json_file)




    # GENERATING DATA

    # Training data will be generated before each training batch
    # Only WEC and WECn first needs to pretrain word embeddings

    if architecture == 'WEC':
        # First gather architecture parameters
        we_corpus = architecture_parameters['we_corpus_size'] 
        we_size = architecture_parameters['we_size']
        we_window = architecture_parameters['we_window']
        we_algorithm = architecture_parameters['we_algorithm']
        load_embeddings_from = architecture_parameters.get(
                'load_embeddings_from', None
            )


        if load_embeddings_from is None:
            if verbose:
                print('Now pretraining word embeddings')

                print("First generate profiles and turn them into corpus")
            # Generate profiles and their winning sets
            X_train_profs, _ , _ = generate_profile_data(
                max_num_voters,
                max_num_alternatives,
                we_corpus,
                election_sampling,
                [],
                merge='empty'
            )

            # Turn set of profiles X into a corpus (each profile a sentence)
            train_sentences = [
                    [models.ranking_to_string(ranking) 
                    for ranking in profile.rankings] 
                for profile in X_train_profs]
            # Add the 'UNK' word for future unknown words. And add 'PAD' for 
            # padding sentences to desired length. (Adding these after training 
            # the embeddings seems inefficient.)
            train_sentences_with_UNK_and_PAD=train_sentences+[['UNK'], ['PAD']]

            # Pretrain an word embedding on this corpus
            if verbose:
                print('Now train word embeddings')
            pre_embeddings = Word2Vec(
                    train_sentences_with_UNK_and_PAD, 
                    vector_size=we_size, 
                    window=we_window, 
                    min_count=1, 
                    workers=8, 
                    sg=we_algorithm
                )
            print('Done pretraining word embeddings.')

            if save_model:
                if verbose:
                    print('Save the word embeddings.')
                # We save the pre_embedding word2vec model, so it can be used
                # when initializing a WEC model that we then load with 
                # previously trained parameters 
                pre_embeddings.save(f"{location}/pre_embeddings.bin")

        # Load back with memory-mapping = read-only, shared across processes.
        if load_embeddings_from is not None:
            print('Load the word embeddings.')
            load_embeddings_from = architecture_parameters['load_embeddings_from']
            pre_embeddings = Word2Vec.load(f"{load_embeddings_from}/pre_embeddings.bin")


    # Dev dataset
    if verbose:
        print('Now generate dev and test profiles')

    X_dev_profs, y_dev_profs, _ = generate_profile_data(
        max_num_voters,
        max_num_alternatives,
        eval_dataset_size,
        election_sampling,
        [utils.dict_rules[rule_name] for rule_name in rule_names],
        merge=merge,
    )

    if architecture == 'MLP':
        dev_dataloader = generate_data.tensorize_profile_data_MLP(
            X_dev_profs,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'CNN':
        dev_dataloader = generate_data.tensorize_profile_data_CNN(
            X_dev_profs,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'WEC':
        dev_sentences = [
                [models.ranking_to_string(ranking)
                for ranking in profile.rankings]
            for profile in X_dev_profs
            ]
        dev_dataloader, summary_unks=generate_data.tensorize_profile_data_WEC(
            pre_embeddings,
            dev_sentences,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size,
            num_of_unks=True
        )
        # Initialize computation of number of UNKs
        number_of_unks = {}
        # Compute number of UNKs in dev set
        ratio, num_unks, all_words = summary_unks['ratio'], summary_unks['num_unks'], summary_unks['all_words']
        print(f'Occurrences of UNK in dev data: {ratio}% ({num_unks} of {all_words} words)')
        number_of_unks['num_unks_dev_set'] = summary_unks 


    X_test_profs, y_test_profs, _ = generate_profile_data(
        max_num_voters,
        max_num_alternatives,
        eval_dataset_size,
        election_sampling,
        [utils.dict_rules[rule_name] for rule_name in rule_names],
        merge=merge,
    )


    if architecture == 'MLP':
        test_dataloader = generate_data.tensorize_profile_data_MLP(
            X_test_profs,
            y_test_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'CNN':
        test_dataloader = generate_data.tensorize_profile_data_CNN(
            X_test_profs,
            y_test_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'WEC':
        test_sentences = [
                [models.ranking_to_string(ranking)
                for ranking in profile.rankings]
            for profile in X_test_profs
            ]
        test_dataloader, summary_unks=generate_data.tensorize_profile_data_WEC(
            pre_embeddings,
            test_sentences,
            y_test_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size,
            num_of_unks=True
        )
        # Compute number of UNKs in test set
        ratio, num_unks, all_words = summary_unks['ratio'], summary_unks['num_unks'], summary_unks['all_words']
        if verbose:
            print(f'Occurrences of UNK in test data: {ratio}% ({num_unks} of {all_words} words)')
        number_of_unks['num_unks_test_set'] = summary_unks 


        with open(f"{location}/results.json") as json_file:
            data = json.load(json_file)

        data.update({"number_of_unks": number_of_unks})

        with open(f"{location}/results.json", "w") as json_file:
            json.dump(data, json_file)






    # NEURAL NETWORK TRAINING

    #Initialize our model for the experiment
    if architecture == 'MLP':
        exp_model = MLP(max_num_voters, max_num_alternatives)
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )

    if architecture == 'CNN':
        exp_model = CNN(
            max_num_voters, 
            max_num_alternatives,
            architecture_parameters['kernel1'],
            architecture_parameters['kernel2'],
            architecture_parameters['channels']
        )
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )


    if architecture == 'WEC':    
        exp_model = WEC(pre_embeddings, max_num_voters, max_num_alternatives)
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )



    # Load the previous state of the model if given
    if load_model_from is not None:
        checkpoint = torch.load(f'{load_model_from}/model.pth')
        exp_model.load_state_dict(checkpoint['model_state_dict'])
        exp_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # Set up the learning rate scheduler if given
    if learning_scheduler is not None:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            exp_optimizer, 
            T_0 = learning_scheduler
        )


    if verbose:
        print('Now starting to train')
    learning_curve = {}
    report_plot = report_intervals['plot']
    report_print = report_intervals['print']

    
    for step in tqdm(range(num_gradient_steps)):

        # Generate data for the batch
        X_train_profs = []
        y_train_wins = []

        # (a) sample proper profiles and their winning sets
        X_proper_profs, y_proper_wins, _ = generate_profile_data(
            max_num_voters,
            max_num_alternatives,
            batch_composition['proper_data'],
            election_sampling,
            [utils.dict_rules[rule_name] for rule_name in rule_names],
            merge=merge,
        )
        X_train_profs += X_proper_profs 
        y_train_wins += y_proper_wins

        # (b) randomly pick a proper profile and an alternatives-permutation 
        # and add the profile and winning sets with alternatives thus permuted 
        for _ in range(batch_composition['neut_generated']):
            # randomly pick a profile and corresponding winning set
            index = random.randrange(len(X_proper_profs))
            prof = X_proper_profs[index]
            winning_set = y_proper_wins[index]
            # randomly pick a permutation of the alternatives in the profile
            p = random.sample(list(range(prof.num_cands)), prof.num_cands)
            # permute the profile 
            permuted_prof = Profile([
                    [p[alt] for alt in ranking] 
                    for ranking in prof.rankings
                ])
            # permute the winning set
            permuted_winning_set = tuple([p[alt] for alt in winning_set])
            # add the permuted profile and winning set to batch
            X_train_profs.append(permuted_prof)
            y_train_wins.append(permuted_winning_set)


        # (c) randomly pick a proper profile and a voter-permutation
        # and add the profile and winning set with voters thus permuted
        for _ in range(batch_composition['anon_generated']):
            # randomly pick a profile and corresponding winning set
            index = random.randrange(len(X_proper_profs))
            prof = X_proper_profs[index]
            winning_set = y_proper_wins[index]
            # permute the profile
            permuted_prof = Profile(random.sample(
                prof.rankings, len(prof.rankings)
            ))
            # add the permuted profile and winning set to batch
            X_train_profs.append(permuted_prof)
            y_train_wins.append(winning_set)


        # Turn profiles and winning sets into training_dataloader
        if architecture == 'MLP':
            train_dataloader = generate_data.tensorize_profile_data_MLP(
                X_train_profs,
                y_train_wins,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'CNN':
            train_dataloader = generate_data.tensorize_profile_data_CNN(
                X_train_profs, 
                y_train_wins,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'WEC':
            train_sentences = [
                [models.ranking_to_string(rank) for rank in prof.rankings]
                for prof in X_train_profs
            ]
            train_dataloader, _ = generate_data.tensorize_profile_data_WEC(
                pre_embeddings,
                train_sentences,
                y_train_wins,
                max_num_voters,max_num_alternatives,batch_size,
                num_of_unks=False,
                shuffle=True
            )

        # Do step in gradient descent
        train_and_eval.train(
            train_dataloader,
            exp_model,
            exp_loss,
            exp_optimizer
        )

        if learning_scheduler is not None:
            scheduler.step()

        # Intermediate performance check

        if step % report_plot == 0:
            dev_accuracy = train_and_eval.accuracy(exp_model, dev_dataloader)
            dev_loss = train_and_eval.loss(exp_model, exp_loss, dev_dataloader)
            learning_curve[f'{step}'] = {'dev_loss' : dev_loss,
                                        'dev_accuracy' : dev_accuracy}
        if step % report_print == 0:
            dev_accuracy = train_and_eval.accuracy(exp_model, dev_dataloader)
            dev_loss = train_and_eval.loss(exp_model, exp_loss, dev_dataloader)
            if learning_scheduler is not None:
                current_learning_rate = scheduler.get_last_lr()[0]
            else:
                current_learning_rate = exp_optimizer.defaults['lr']
            if verbose:
                print(f'Step {step}: dev-loss {round(dev_loss,5)}, dev-accuracy {dev_accuracy}, lr={round(current_learning_rate,5)}')



    num_params = summary(exp_model).total_params
    if verbose:
        print(f'Done training. Number of model parameters: {num_params}')

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({
        "learning curve": learning_curve, 
        "number_of_model_parameters":num_params
    })

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)

    if save_model:
        # We save both the model state and the optimizer state to be able to
        # continue training later on.
        torch.save({
            'arguments' : [
                max_num_voters, 
                max_num_alternatives, 
                architecture_parameters
            ],
            'model_state_dict': exp_model.state_dict(),
            'optimizer_state_dict': exp_optimizer.state_dict()
            }, f"{location}/model.pth")





    # EVALUATION

    # Test accuracy

    test_accuracy = train_and_eval.accuracy(exp_model, test_dataloader)
    print(f'Test-accuracy: {test_accuracy}')
    test_results = {'accuracy_on_test_data':test_accuracy}


    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"test_accuracy":test_results})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)



    # Define the rule computed by the model
    if architecture == 'MLP':
        model_rule = models.MLP2rule(exp_model)
        model_rule_full = models.MLP2rule(exp_model, full=True)

    if architecture == 'CNN':
        model_rule = models.CNN2rule(exp_model)
        model_rule_full = models.CNN2rule(exp_model, full=True)

    if architecture == 'WEC':    
        model_rule = models.WEC2rule(exp_model)
        model_rule_full = models.WEC2rule(exp_model, full=True)


    # Admissibility
    admissibility_summary = train_and_eval.admissibility(
        model_rule_full,
        X_test_profs
    )
            

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"admissability": admissibility_summary})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)


    # Rule similarity
    rule_similarity = train_and_eval.rule_similarity(
        model_rule,
        rule_names,
        X_test_profs,
    )         

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"rule_similarity": rule_similarity})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)


    
    # Initialize dictionary with axiom satisfactions
    axiom_satisfactions = {}

    # Axiom satisfaction of model
    print('Axiom satisfaction (model):')
    axiom_satisfaction_model = {}
    for axiom_name in axioms_check_model:
        sat = train_and_eval.axiom_satisfaction(model_rule,
                utils.dict_axioms[axiom_name],
                max_num_voters,
                max_num_alternatives,
                election_sampling,
                sample_size_applicable,
                sample_size_maximal,
                utils.dict_axioms_sample[axiom_name],
                full_profile=False,
                comparison_rule=None)
        axiom_satisfaction_model[axiom_name] = sat
        cond_sat = sat['cond_satisfaction']
        print(f'    {axiom_name} {100*cond_sat}%')
    axiom_satisfactions['learned_rule'] = axiom_satisfaction_model

    # Axiom satisfaction of rules
    for rule_name in rule_names:
        if verbose:
            print(f'Axiom satisfaction ({rule_name}):')
        axiom_satisfaction_current_rule = {}
        for axiom_name in axioms_check_rule:
            sat = train_and_eval.axiom_satisfaction(
                utils.dict_rules[rule_name],
                utils.dict_axioms[axiom_name],
                max_num_voters,
                max_num_alternatives,
                election_sampling,
                sample_size_applicable,
                sample_size_maximal,
                utils.dict_axioms_sample[axiom_name],
                full_profile=False,
                comparison_rule=None
            )
            axiom_satisfaction_current_rule[axiom_name] = sat
            cond_sat = sat['cond_satisfaction']
            print(f'    {axiom_name} {100*cond_sat}%')
        axiom_satisfactions[rule_name] = axiom_satisfaction_current_rule

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"axiom_satisfaction": axiom_satisfactions})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)


    
    
    # Comparison rules
    if comparison_rules is not None:
        similarities = train_and_eval.rule_similarity(
            model_rule, 
            comparison_rules, 
            X_test_profs
        )
        if verbose:
            print('Similarity to comparison rules:')
            for rule_name in comparison_rules:
                coinc = 100*similarities[rule_name]["identity_accu"]
                print(f'    {rule_name} {coinc}%')
    
        with open(f"{location}/results.json") as json_file:
            data = json.load(json_file)

        data.update({"rule_comparison": similarities})

        with open(f"{location}/results.json", "w") as json_file:
            json.dump(data, json_file)

    # Resoluteness
    if compute_resoluteness:

        resoluteness = {}

        # Resoluteness of model
        res_coefficient_of_model = train_and_eval.resoluteness(model_rule, X_test_profs)
        if verbose:
            print(f'The learned rule has resoluteness {res_coefficient_of_model}')
        resoluteness['learned_rule'] = res_coefficient_of_model

        # Resoluteness of rules
        for rule_name in rule_names:
            res_coefficient_of_current_rule = train_and_eval.resoluteness(utils.dict_rules[rule_name], X_test_profs)
            if verbose:
                print(f'Rule {rule_name} has resoluteness {res_coefficient_of_current_rule}')
            resoluteness[rule_name] = res_coefficient_of_current_rule

        with open(f"{location}/results.json") as json_file:
            data = json.load(json_file)

        data.update({"resoluteness": resoluteness})

        with open(f"{location}/results.json", "w") as json_file:
            json.dump(data, json_file)
   



    end_time = time.time()
    duration = end_time - start_time

    print(f'Runtime (in min): {round(duration/60)}')

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"runtime_in_sec": duration})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)


    return location









def experiment2_straightforward(
        architecture,
        rule_names,
        max_num_voters,
        max_num_alternatives,
        election_sampling,
        considered_axiom,
        initial_gradient_steps,
        continued_gradient_steps,
        eval_dataset_size,
        sample_size_applicable,
        sample_size_maximal,
        architecture_parameters=None,
        axioms_check_model = list(utils.dict_axioms.keys()),
        merge="accumulative",
        random_seed=None,
        report_interval = 1000,
        batch_size=200,
        learning_rate = 1e-3,
        learning_scheduler = None,
        weight_decay = 0,
        save_model=False,
        verbose=False,
    ):
    """
    Implements a straightforward version of experiment 2 

    Inputs:
    * `architecture` can be either `MLP`, `CNN`, or `WEC`. The latter two 
      require additional parameters which can be passed as 
      `architecture_parameters` below (otherwise default values are chosen).

    * `rule_names`: list of names of rules used for data generation. Typically 
      just a singleton list. If multiple rules are given, their output is 
      merged in the dataset. 
    * `max_num_voters`: a positive integer describing the maximal number of 
      voters that will be considered
    * `max_num_alternatives`: a positive integer describing the maximal number 
      of alternatives that will be considered
    * `election_sampling`: a dictionary describing the parameters for the
      probability model with which profiles are generated. The most important 
      key is `probmodel`. See 
      https://pref-voting.readthedocs.io/en/latest/generate_profiles.html


    * `considered_axiom` is either 'Neutrality' or 'Anonymity'.    
    * `initial_gradient_steps` is the number of training steps done with the 
      initially sampled data. After this a copy of the model is made.
    * `continued_gradient_steps` is the number of training steps done on the 
      original and the copied model. The original model is trained with sampled
      data, the copied model is trained with data obtained by 
      `considered_axiom` variations of the initially sampled training data.
      (For this, one randomly picks a profile and an alternatives/voter 
      permutation and adds the profile and winning sets with 
      alternatives/voters thus permuted. 

    * `eval_dataset_size`: a positive integer describing the number of profiles
      with their corresponding winning sets that should be used for testing.
    * `sample_size_applicable`: a positive integer describing the number of 
      sampled profiles on which the axioms are checked and applicable. 
    * `sample_size_maximal`: a positive integer describing how many profiles 
      are at most sampled when trying to find profiles on which the axioms are 
      applicable. 

    * `architecture_parameters` is not needed for MLPs, but CNNs and WEC they 
       are given the following default values
        * For CNN: 
            {
                'kernel1':[5,1], 
                'kernel2':[1,5], 
                'channels':32
            } 
          Here `kernel1`: a list [height, width] of the dimension for the 
          kernel/filter in the first convolutional layer of the model. (The 
          height of the images is max_num_alternatives and the width of the 
          images is max_num_voters.) If max num voters/alternatives are quite 
          small, the kernel cannot be too big, otherwise error will be raised.  
          Similarly, `kernel2` is the [height, width]-dimension for the 
          kernel/filter in the second convolutional layer of the model.
          Finally, `channels` is the number of channels of the feature maps of 
          the model.
        * For WEC: 
            {
                'we_corpus_size':int(1e5), 
                'we_size':100, 
                'we_window':5, 
                'we_algorithm':1,
            } 
          Here 'we_corpus_size' is the number of profiles used to pretrain the 
          word embeddings,  `we_size` is the size (i.e., length of vector) of 
          the word embeddings, `we_window` is the size of the window used when 
          training the word embeddings. Finally, `we_algorithm` is either 0 
          or 1 depending on whether one uses the CBOW algorithm or the 
          skip gram algorithm.  If 'load_embeddings_from' is specified with a 
          path to stored embeddings, then these are loaded rather than 
          computing new ones. 
              
    * `axioms_check_model` is a list of names of axioms whose satisfaction is 
      checked for the trained model. By default, it is set to all axioms. If 
      empty, no axioms are checked.
    * `merge`: By default set to `accumulative`, but could also be `selective`. 
      If `rule_names` has length 1 (i.e., only a single rule is considered), 
      both are equivalent. If there are several rules, `selective` means a 
      profile is added to the training dataset only if all rules output the 
      same winning set, while with `accumulative` all profiles are added.
    
    * `random_seed` (optional): If not None, set all random seeds to this 
      provided value.
    * `report_interval` is by default 1000, saying that after every 1000 
      gradient steps the accuracy and axiom satisfaction is computed on the 
      dev-dataset.
    * `batch_size`: a positive integer describing the size of the batches when 
      training the model. The default is 200.
    * `learning_rate`: a float number describing the learning rate when 
      training the model. The default value is 1e-3.
    * `learning_scheduler`: By default None, but if given, then a positive 
      integer describing the T_0 value for the CosineAnnealingWarmRestarts 
      scheduler (i.e., the number of iterations for the first restart).
    * `weight_decay` of the optimizer which we set by default to 0 since we use 
      synthetic data and hence don't need regularization (its usual default 
      value is 0.01).  
    * `save_model`: By default False, but if true, the neural network model 
      will be saved.
    """


    # SET UP BASICS

    start_time = time.time()

    assert (
        architecture in ['MLP', 'CNN', 'WEC']
    ), f"The supported architectures are 'MLP', 'CNN', and 'WEC' but {architecture} was given"

    assert (
        considered_axiom in ['Neutrality', 'Anonymity']
    ), f"The supported axioms are Neutrality and Anonymity but {considered_axiom} was given"


    if architecture_parameters is None:
        if architecture == 'CNN':
            architecture_parameters = {
                'kernel1':[5,1] , 
                'kernel2':[1,5], 
                'channels':32} 
        if architecture == 'WEC':
            architecture_parameters = {
                'we_corpus_size':int(1e5),
                'we_size':100, 
                'we_window':5, 
                'we_algorithm':1}



    # Set seeds
    if random_seed is not None:
        random.seed(random_seed)
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        torch.cuda.manual_seed(random_seed)
        torch.use_deterministic_algorithms(True)
        torch.backends.cudnn.benchmark = False

    # Set up saving of results
    rules_short = ""
    for rule_name in rule_names:
        rules_short += rule_name
    prob_model = election_sampling['probmodel']
    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    location = f"./results/exp2/{architecture}/exp2_straight_{current_time}_{rules_short}_{prob_model}"
    os.mkdir(location)
    print(f'Saving location: {location}')


    results = {
        "location": location,        
        "architecture": architecture,
        "rule_names": rule_names,
        "max_num_voters": max_num_voters,
        "max_num_alternatives": max_num_alternatives,
        "election_sampling": election_sampling,
        "considered_axiom": considered_axiom,
        "initial_gradient_steps": initial_gradient_steps,
        "continued_gradient_steps": continued_gradient_steps,
        "eval_dataset_size": eval_dataset_size,
        "sample_size_applicable": sample_size_applicable,
        "sample_size_maximal": sample_size_maximal,
        "architecture_parameters": architecture_parameters,
        "axioms_check_model": axioms_check_model,
        "merge": merge,
        "random_seed": random_seed,
        "report_interval": report_interval,
        "batch_size": batch_size,
        "learning_rate": learning_rate,
        "learning_scheduler": learning_scheduler,
        "weight_decay": weight_decay,
        "save_model": save_model,
    }



    with open(f"{location}/results.json", "w") as json_file:
        json.dump(results, json_file)




    # GENERATING DATA

    # Training data will be generated before each training batch
    # Only WEC and WECn first needs to pretrain word embeddings

    if architecture == 'WEC':
        # First gather architecture parameters
        we_corpus = architecture_parameters['we_corpus_size'] 
        we_size = architecture_parameters['we_size']
        we_window = architecture_parameters['we_window']
        we_algorithm = architecture_parameters['we_algorithm']
        load_embeddings_from = architecture_parameters.get(
                'load_embeddings_from', None
            ) 


        if load_embeddings_from is None:
            if verbose:
                print('Now pretraining word embeddings')

                print("First generate profiles and turn them into corpus")
            # Generate profiles and their winning sets
            X_train_profs, _ , _ = generate_profile_data(
                max_num_voters,
                max_num_alternatives,
                we_corpus,
                election_sampling,
                [],
                merge='empty'
            )

            # Turn set of profiles X into a corpus (each profile a sentence)
            train_sentences = [
                    [models.ranking_to_string(ranking) 
                    for ranking in profile.rankings] 
                for profile in X_train_profs]
            # Add the 'UNK' word for future unknown words. And add 'PAD' for 
            # padding sentences to desired length. (Adding these after training 
            # the embeddings seems inefficient.)
            train_sentences_with_UNK_and_PAD=train_sentences+[['UNK'], ['PAD']]

            # Pretrain an word embedding on this corpus
            if verbose:
                print('Now train word embeddings')
            pre_embeddings = Word2Vec(
                    train_sentences_with_UNK_and_PAD, 
                    vector_size=we_size, 
                    window=we_window, 
                    min_count=1, 
                    workers=8, 
                    sg=we_algorithm
                )
            print('Done pretraining word embeddings.')

            if save_model:
                if verbose:
                    print('Save the word embeddings.')
                # We save the pre_embedding word2vec model, so it can be used
                # when initializing a WEC model that we then load with 
                # previously trained parameters 
                pre_embeddings.save(f"{location}/pre_embeddings.bin")

        # Load back with memory-mapping = read-only, shared across processes.
        if load_embeddings_from is not None:
            print('Load the word embeddings.')
            load_embeddings_from = architecture_parameters['load_embeddings_from']
            pre_embeddings = Word2Vec.load(f"{load_embeddings_from}/pre_embeddings.bin")


    # Dev dataset
    if verbose:
        print('Now generate dev and test profiles')

    X_dev_profs, y_dev_profs, _ = generate_profile_data(
        max_num_voters,
        max_num_alternatives,
        eval_dataset_size,
        election_sampling,
        [utils.dict_rules[rule_name] for rule_name in rule_names],
        merge=merge,
    )

    if architecture == 'MLP':
        dev_dataloader = generate_data.tensorize_profile_data_MLP(
            X_dev_profs,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'CNN':
        dev_dataloader = generate_data.tensorize_profile_data_CNN(
            X_dev_profs,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size
        )

    if architecture == 'WEC':
        dev_sentences = [
                [models.ranking_to_string(ranking)
                for ranking in profile.rankings]
            for profile in X_dev_profs
            ]
        dev_dataloader, summary_unks=generate_data.tensorize_profile_data_WEC(
            pre_embeddings,
            dev_sentences,
            y_dev_profs,
            max_num_voters,
            max_num_alternatives,
            batch_size,
            num_of_unks=True
        )
        # Initialize computation of number of UNKs
        number_of_unks = {}
        # Compute number of UNKs in dev set
        ratio, num_unks, all_words = summary_unks['ratio'], summary_unks['num_unks'], summary_unks['all_words']
        print(f'Occurrences of UNK in dev data: {ratio}% ({num_unks} of {all_words} words)')
        number_of_unks['num_unks_dev_set'] = summary_unks 







    # NEURAL NETWORK TRAINING

    #Initialize our model for the experiment
    if architecture == 'MLP':
        exp_model = MLP(max_num_voters, max_num_alternatives)
        exp_model_copy = MLP(max_num_voters, max_num_alternatives)
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        exp_optimizer_copy = torch.optim.AdamW(
            exp_model_copy.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )

    if architecture == 'CNN':
        exp_model = CNN(
            max_num_voters, 
            max_num_alternatives,
            architecture_parameters['kernel1'],
            architecture_parameters['kernel2'],
            architecture_parameters['channels']
        )
        exp_model_copy = CNN(
            max_num_voters, 
            max_num_alternatives,
            architecture_parameters['kernel1'],
            architecture_parameters['kernel2'],
            architecture_parameters['channels']
        ) 
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        exp_optimizer_copy = torch.optim.AdamW(
            exp_model_copy.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )


    if architecture == 'WEC':
        exp_model = WEC(
            pre_embeddings,
            max_num_voters,
            max_num_alternatives
        )
        exp_model_copy = WEC(
            pre_embeddings,
            max_num_voters,
            max_num_alternatives
        )
        exp_loss = nn.BCEWithLogitsLoss()
        exp_optimizer = torch.optim.AdamW(
            exp_model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        exp_optimizer_copy = torch.optim.AdamW(
            exp_model_copy.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )



    # Set up the learning rate scheduler if given
    if learning_scheduler is not None:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            exp_optimizer,
            T_0 = learning_scheduler
        )


    if verbose:
        print('Now starting to train')
    
    learning_curve = {}

    # Initial training
    num_initial_data = initial_gradient_steps * batch_size
    X_train_initial, y_train_initial, _ = generate_profile_data(
        max_num_voters,
        max_num_alternatives,
        num_initial_data,
        election_sampling,
        [utils.dict_rules[rule_name] for rule_name in rule_names],
        merge=merge,
    )

    # Turn profiles and winning sets into training_dataloader
    if architecture == 'MLP':
        train_dataloader = generate_data.tensorize_profile_data_MLP(
            X_train_initial, 
            y_train_initial,
            max_num_voters,
            max_num_alternatives,
            batch_size,
            shuffle=True
        )

    if architecture == 'CNN':
        train_dataloader = generate_data.tensorize_profile_data_CNN(
            X_train_initial,
            y_train_initial,
            max_num_voters,
            max_num_alternatives,
            batch_size,
            shuffle=True
        )

    if architecture == 'WEC':
        train_sentences = [
            [models.ranking_to_string(rank) for rank in prof.rankings]
            for prof in X_train_initial
        ]
        train_dataloader, _ = generate_data.tensorize_profile_data_WEC(
            pre_embeddings,
            train_sentences,
            y_train_initial,
            max_num_voters,max_num_alternatives,batch_size,
            num_of_unks=False,
            shuffle=True
        )

    # Do one epoch of training on the dataloader
    exp_model.train()
    for batch, (x, y) in enumerate(train_dataloader):
        # Compute prediction as logits
        logits = exp_model(x)
        # Compute error the prediction
        loss = exp_loss(logits, y)
        # Backpropagation
        exp_optimizer.zero_grad()
        loss.backward()
        exp_optimizer.step()
        if learning_scheduler is not None:
            scheduler.step()
    
    # Now copy the model
    exp_model_copy.load_state_dict(exp_model.state_dict())
    exp_optimizer_copy.load_state_dict(exp_optimizer.state_dict())

    
    for step in tqdm(range(continued_gradient_steps)):

        # PART 1 Train original model on augmented data
        
        # Create batch
        X_train = []
        y_train = []

        if considered_axiom == 'Neutrality':
            # randomly pick an initial profile and an alternatives-permutation
            # and add the thus permuted profile and winning set
            for _ in range(batch_size):
                # randomly pick a profile and corresponding winning set
                index = random.randrange(num_initial_data)
                prof = X_train_initial[index]
                winning_set = y_train_initial[index]
                # randomly pick a permutation of alternatives in the profile
                p = random.sample(list(range(prof.num_cands)), prof.num_cands)
                # permute the profile
                permuted_prof = Profile([
                        [p[alt] for alt in ranking]
                        for ranking in prof.rankings
                    ])
                # permute the winning set
                permuted_winning_set = tuple([p[alt] for alt in winning_set])
                # add the permuted profile and winning set to batch
                X_train.append(permuted_prof)
                y_train.append(permuted_winning_set)

        if considered_axiom == 'Anonymity':
        # randomly pick an initial profile and a voter-permutation
        # and add the thus permuted profile and winning set
            for _ in range(batch_size):
                # randomly pick a profile and corresponding winning set
                index = random.randrange(num_initial_data)
                prof = X_train_initial[index]
                winning_set = y_train_initial[index]
                # permute the profile
                permuted_prof = Profile(random.sample(
                    prof.rankings, len(prof.rankings)
                ))
                # add the permuted profile and winning set to batch
                X_train.append(permuted_prof)
                y_train.append(winning_set)


        # Turn profiles and winning sets into training_dataloader
        if architecture == 'MLP':
            train_dataloader = generate_data.tensorize_profile_data_MLP(
                X_train,
                y_train,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'CNN':
            train_dataloader = generate_data.tensorize_profile_data_CNN(
                X_train,
                y_train,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'WEC':
            train_sentences = [
                [models.ranking_to_string(rank) for rank in prof.rankings]
                for prof in X_train
            ]
            train_dataloader, _ = generate_data.tensorize_profile_data_WEC(
                pre_embeddings,
                train_sentences,
                y_train,
                max_num_voters,max_num_alternatives,batch_size,
                num_of_unks=False,
                shuffle=True
            )

        # Do step in gradient descent
        train_and_eval.train(
            train_dataloader,
            exp_model,
            exp_loss,
            exp_optimizer
        )
        
        

        # PART 2 Train copied model on sampled data
        
        # Create batch
        X_train, y_train, _ = generate_profile_data(
            max_num_voters,
            max_num_alternatives,
            batch_size,
            election_sampling,
            [utils.dict_rules[rule_name] for rule_name in rule_names],
            merge=merge,
        )

        # Turn profiles and winning sets into training_dataloader
        if architecture == 'MLP':
            train_dataloader = generate_data.tensorize_profile_data_MLP(
                X_train, 
                y_train,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'CNN':
            train_dataloader = generate_data.tensorize_profile_data_CNN(
                X_train, 
                y_train,
                max_num_voters,
                max_num_alternatives,
                batch_size,
                shuffle=True
            )

        if architecture == 'WEC':
            train_sentences = [
                [models.ranking_to_string(rank) for rank in prof.rankings]
                for prof in X_train
            ]
            train_dataloader, _ = generate_data.tensorize_profile_data_WEC(
                pre_embeddings,
                train_sentences,
                y_train,
                max_num_voters,max_num_alternatives,batch_size,
                num_of_unks=False,
                shuffle=True
            )

        # Do step in gradient descent
        train_and_eval.train(
            train_dataloader, 
            exp_model_copy, 
            exp_loss, 
            exp_optimizer_copy
        )

        # Do a single step in scheduler for both 
        if learning_scheduler is not None:
            scheduler.step()




        # Intermediate performance check

        if step % report_interval == 0:
            
            # Dev accuracy
            dev_accuracy_orig = train_and_eval.accuracy(
                exp_model,
                dev_dataloader
            )
            dev_loss_orig = train_and_eval.loss(
                exp_model,
                exp_loss,
                dev_dataloader
            )
            dev_accuracy_copy = train_and_eval.accuracy(
                exp_model_copy,
                dev_dataloader
            )
            dev_loss_copy = train_and_eval.loss(
                exp_model_copy,
                exp_loss,
                dev_dataloader
            )

            # Dev axiom satisfaction

            # Define the rule computed by the model
            if architecture in ['MLP']:
                model_rule_orig = models.MLP2rule(exp_model)
                model_rule_full_orig = models.MLP2rule(exp_model, full=True)
                model_rule_copy = models.MLP2rule(exp_model_copy)
                model_rule_full_copy=models.MLP2rule(exp_model_copy, full=True)

            if architecture == 'CNN':
                model_rule_orig = models.CNN2rule(exp_model)
                model_rule_full_orig = models.CNN2rule(exp_model, full=True)
                model_rule_copy = models.CNN2rule(exp_model_copy)
                model_rule_full_copy=models.CNN2rule(exp_model_copy, full=True)                

            if architecture == 'WEC':    
                model_rule_orig = models.WEC2rule(exp_model)
                model_rule_full_orig = models.WEC2rule(exp_model, full=True)
                model_rule_copy = models.WEC2rule(exp_model_copy)
                model_rule_full_copy=models.WEC2rule(exp_model_copy, full=True)


            # Admissibility
            admissibility_summary_orig = train_and_eval.admissibility(
                model_rule_full_orig,
                X_dev_profs
            )
            admissibility_summary_copy = train_and_eval.admissibility(
                model_rule_full_copy,
                X_dev_profs
            )

           
            # Axiom satisfaction
            axiom_satisfaction_orig = {}
            for axiom_name in axioms_check_model:
                sat = train_and_eval.axiom_satisfaction(
                    model_rule_orig,
                    utils.dict_axioms[axiom_name],
                    max_num_voters,
                    max_num_alternatives,
                    election_sampling,
                    sample_size_applicable,
                    sample_size_maximal,
                    utils.dict_axioms_sample[axiom_name], 
                    full_profile=False,
                    comparison_rule=None)
                axiom_satisfaction_orig[axiom_name] = sat

            axiom_satisfaction_copy = {}
            for axiom_name in axioms_check_model:
                sat = train_and_eval.axiom_satisfaction(
                    model_rule_copy,
                    utils.dict_axioms[axiom_name],
                    max_num_voters,
                    max_num_alternatives,
                    election_sampling,
                    sample_size_applicable,
                    sample_size_maximal,
                    utils.dict_axioms_sample[axiom_name],
                    full_profile=False,
                    comparison_rule=None)
                axiom_satisfaction_copy[axiom_name] = sat

            # Gather everything
            learning_curve[f'{step}'] = {
                'dev_accuracy_orig' : dev_accuracy_orig,
                'dev_loss_orig' : dev_loss_orig,
                'dev_accuracy_copy' : dev_accuracy_copy,
                'dev_loss_copy' : dev_loss_copy,
                'dev_admissibility_orig' : admissibility_summary_orig,
                'dev_admissibility_copy' : admissibility_summary_copy,
                'dev_axiom_satisfaction_orig' : axiom_satisfaction_orig,
                'dev_axiom_satisfaction_copy' : axiom_satisfaction_copy,
            }




    num_params = summary(exp_model).total_params #same for orig and copy
    if verbose:
        print(f'Done training. Number of model parameters: {num_params}')

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({
        "learning curve": learning_curve, 
        "number_of_model_parameters":num_params
    })

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)

    if save_model:
        # We save both the model state and the optimizer state to be able to
        # continue training later on.
        torch.save({
            'arguments' : [
                max_num_voters, 
                max_num_alternatives, 
                architecture_parameters
            ],
            'model_state_dict': exp_model.state_dict(),
            'optimizer_state_dict': exp_optimizer.state_dict()
            }, f"{location}/model_orig.pth")
        
        torch.save({
            'arguments' : [
                max_num_voters, 
                max_num_alternatives, 
                architecture_parameters
            ],
            'model_state_dict': exp_model_copy.state_dict(),
            'optimizer_state_dict': exp_optimizer_copy.state_dict()
            }, f"{location}/model_copy.pth")



    end_time = time.time()
    duration = end_time - start_time

    print(f'Runtime (in min): {round(duration/60)}')

    with open(f"{location}/results.json") as json_file:
        data = json.load(json_file)

    data.update({"runtime_in_sec": duration})

    with open(f"{location}/results.json", "w") as json_file:
        json.dump(data, json_file)


    return location